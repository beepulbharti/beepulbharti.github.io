---
---
@inproceedings{bharti2025multiaccuracy,
  title={Multiaccuracy and Multicalibration via Proxy Groups},
  author={Bharti, Beepul and Clemens-Sewall, Mary Versa, and Yi, Paul and Sulam, Jeremias},
  booktitle={Forty-second International Conference on Machine Learning},
  year={2025},
  selected={true},
  abbr={ICML},
  date={May. 01, 2025},
  url={https://openreview.net/forum?id=sGny74zx2V},
  abstract={As the use of predictive machine learning algorithms increases in high-stakes decision-making, it is imperative that these algorithms are fair across sensitive groups. However, measuring and enforcing fairness in real-world applications can be challenging due to missing or incomplete sensitive group data. Proxy-sensitive attributes have been proposed as a practical and effective solution in these settings, but only for parity-based fairness notions. Knowing how to evaluate and control for fairness with missing sensitive group data for newer and more flexible frameworks, such as multiaccuracy and multicalibration, remains unexplored. In this work, we address this gap by demonstrating that in the absence of sensitive group data, proxy-sensitive attributes can provably be used to derive actionable upper bounds on the true multiaccuracy and multicalibration, providing insights into a model’s potential worst-case fairness violations. Additionally, we show that adjusting models to satisfy multiaccuracy and multicalibration across proxy-sensitive attributes can significantly mitigate these violations for the true, but unknown, demographic groups. Through several experiments on real-world datasets, we illustrate that approximate multiaccuracy and multicalibration can be achieved even when sensitive group data is incomplete or unavailable.}
}

@article{bharti2025sufficient,
  title={Sufficient and Necessary Explanations (and What Lies in Between)},
  author={Beepul Bharti and Paul Yi and Jeremias Sulam},
  booktitle={The Second Conference on Parsimony and Learning (Proceedings Track)},
  year={2025},
  selected={true},
  abbr={CPAL},
  date={March. 12, 2025},
  url={https://openreview.net/forum?id=H43BmpeJII},
  abstract={As complex machine learning models continue to be used in high-stakes decision settings, understanding their predictions is crucial. Post-hoc explanation methods aim to identify which features of an input x are important to a model's prediction f(x). However, explanations often vary between methods and lack clarity, limiting the information we can draw from them. To address this, we formalize two precise concepts—sufficiency and necessity—to quantify how features contribute to a model's prediction. We demonstrate that, although intuitive and simple, these two types of explanations may fail to fully reveal which features a model deems important. To overcome this, we propose and study a unified notion of importance that spans the entire sufficiency-necessity axis. Our unified notion, we show, has strong ties to notions of importance based on conditional independence and Shapley values. Lastly, through various experiments, we quantify the sufficiency and necessity of popular post-hoc explanation methods. Furthermore, we show that generating explanations along the sufficiency-necessity axis can uncover important features that may otherwise be missed, providing new insights into feature importance.}
}

@article{bharti2023estimating,
  title        = {Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors},
  author       = {Bharti, Beepul and Yi, Paul and Sulam, Jeremias},
  booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
  year         = {2023},
  selected     = {true},
  abbr         = {NeurIPS},
  date         = {Dec. 13, 2023},
  url          = {https://nips.cc/virtual/2023/poster/70984},
  abstract     = {As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. To do so, one naturally requires access to sensitive attributes, such as demographics, biological sex, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable. In this work we study the well known equalized odds (EOD) definition of fairness. In a setting without sensitive attributes, we first provide tight and computable upper bounds for the EOD violation of a predictor. These bounds precisely reflect the worst possible EOD violation. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is -- and when is not -- optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets.}
}

@article{bharti2023shapley,
  title        = {SHAP-XRT: The Shapley Value Meets Conditional Independence Testing},
  author       = {Teneggi, Jacopo* and Bharti, Beepul* and Romano, Yaniv and Sulam, Jeremias},
  booktitle    = {Transactions of Machine Learning Research},
  year         = {2023},
  selected     = {true},
  abbr         = {TMLR},
  date         = {Dec. 12, 2023},
  url          = {https://openreview.net/forum?id=WFtTpQ47A7},
  abstract     = {The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value---a solution concept from game theory---is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the SHAPley-EXplanation Randomization Test (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley value provide lower and upper bounds to the p-values of their respective tests. Furthermore, we show that the Shapley value itself provides an upper bound to the p-value of a global (i.e., overall) null hypothesis. As a result, we further our understanding of Shapley-based explanation methods from a novel perspective and characterize under which conditions one can make statistically valid claims about feature importance via the Shapley value.}
}

@article{li2023sex,
  title={Sex imbalance produces biased deep learning models for knee osteoarthritis detection},
  author={Li, David and Bharti, Beepul and Wei, Jinchi and Sulam, Jeremias and Yi, Paul H},
  journal={Canadian Association of Radiologists Journal},
  pages={219--221},
  year={2023},
  selected={true},
  abbr={CAR},
  date={Aug. 30, 2022},
  url={https://journals.sagepub.com/doi/full/10.1177/08465371221120539?casa_token=ZJFwAIVn9sYAAAAA:RBdW9octj6zH0CLSmGytB0d00v_Kzp7Qyr091zN-u88DDAu80gSKuP2pQJ8EE_fBUjMXCYz9WB6v2Q},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

